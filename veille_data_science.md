# VEILLE SCIENCE DES DONNEES

# Notions Mathématiques 
# Un vecteur est un élément d'un espace vectoriel, c'est-à-dire qu'il est possible d'effectuer les opérations d'addition et de multiplication par un scalaire (par un nombre), et que ces opérations ont de bonnes propriétés

# les matrices sont des tableaux d'éléments (nombres, caractères) qui servent à interpréter en termes calculatoires, et donc opérationnels.Autrement, une matrice à m lignes et n colonnes est un tableau rectangulaire de m × n nombres, rangés ligne par ligne.Il y a m lignes, et dans chaque ligne n éléments.

# Une variable indépendante est une variable dont la variation influence la valeur des variables dépendantes. La variable dépendante représente ce que l'on cherche à mesurer dans une expérience ou à évaluer dans une équation mathématique, alors que les variables indépendantes sont les éléments  indispensables au calcul.

# La probabilité d'un événement caractérise la possibilité qu'il se produise. Probabilité d’un événement = nombre d'issues favorables (nombre d'éventualités de l'événement) / nombre d'issues possibles (nombre total d'éventualités)

# espérance : l'espérance mathématique d'une variable aléatoire réelle est, intuitivement, la valeur que l'on s'attend à trouver, en moyenne, si l'on répète un grand nombre de fois la même expérience aléatoire. Elle se note {E} (X)} et se lit « espérance de X ». L'espérance d'une série statistique est la moyenne des valeurs de cette série statistique.

# variance :C'est une mesure de la dispersion des valeurs d'un échantillon ou d'une distribution de probabilité

# Ecart-type : l’écart type est une mesure de la dispersion des valeurs d'un échantillon statistique ou d'une distribution de probabilité. Il est défini comme la racine carrée de la variance ou, de manière équivalente, comme la moyenne quadratique des écarts par rapport à la moyenne. 

# La corrélation entre plusieurs variables aléatoires ou statistiques est une notion de liaison qui contredit leur indépendance.Cette corrélation est très souvent réduite à la corrélation linéaire entre variables quantitatives, c’est-à-dire l’ajustement d’une variable par rapport à l’autre par une relation affine obtenue par régression linéaire.

# Moyenne : paramètre statistique, un indicateur pour résumer l'information fournie par un ensemble de données statistiques : elle est égale à la somme de ces données divisée par leur nombre. 

# Médiane : C'est la valeur qui sépare la moitié inférieure de la moitié supérieure d'un ensemble (échantillon, population, distribution de probabilités). C'est un indicateur de tendance centrale de la série.Pour trouver la médiane, il faut classer les valeurs du plus petit au plus grand.Il suffit de regarder ou se trouve le chiffre médian dans une distribution.

# un maximum :  La valeur « maximum » en statistique est la plus grande valeur que l'on retrouve dans une population.

# un minimum : La valeur « minimum » en statistique est la plus petite valeur que l'on retrouve dans une population

# Les quantiles divisent la distribution en plusieurs secteurs d'intérêt. Les quantiles habituellement calculés sont les quartiles :
Q1: 25% des valeurs sont inférieures au premier quartile

Q2 ou médiane: 50% des valeurs sont inférieures au deuxième quartile

Q3 : 75% des valeurs sont inférieures au troisième quartile

Il est possible de couper la distribution en : Quartiles,Déciles,Centiles,Percentiles particuliers

# Boxplot :C'est un graphique simple composé d'un rectangle duquel deux droites sortent afin de représenter certains éléments des données.La valeur centrale du graphique est la médiane.Les bords du rectangle sont les quartiles.Il est intéressant d'utiliser les box-plot lorsqu'on désire visualiser des conepts tels que la symétrie, la dispersion ou la centralité de la distribution des valeurs associées à une variable.

# L'histogramme est un outil fréquemment utilisé pour résumer des données discrètes ou continues qui sont présentées par intervalles de valeurs.Il est souvent employé pour montrer les caractéristiques principales de la distribution des données de façon pratique.

# En optimisation mathématique et en théorie de la décision,  une fonction de coût (parfois aussi appelée fonction d'erreur) est une fonction qui met en correspondance un événement ou les valeurs d'une ou plusieurs variables avec un nombre réel représentant intuitivement un certain "coût" associé à l'événement. Un problème d'optimisation cherche à minimiser une fonction de perte.

# En mathématiques, la dérivée d'une fonction d'une variable réelle mesure l'ampleur du changement de la valeur de la fonction (valeur de sortie) par rapport à un petit changement de son argument (valeur d'entrée).

# La Descente de Gradient est un algorithme d'optimisation qui permet de trouver le minimum de n'importe quelle fonction convexe en convergeant progressivement vers celui-ci. L'algorithme est extrémement puissant et permet d’entraîner les modèles de régression linéaire, régression logistiques ou encore les réseaux de neurones.

# En statistique, les équations normales sont des équations matricielles de la forme :
tAAx = tAb où

A est une matrice de réels de dimensions n×p ;
tA est la matrice transposée de A ;
x est un vecteur réel inconnu de dimension p ;
b est un vecteur connu de dimension n.
Elles sont utilisées pour effectuer une régression linéaire par la méthode des moindres carrés

# Une loi normale est une loi de probabilité absolument continue qui dépend de deux paramètres : son espérance, un nombre réel noté μ, et son écart type,un nombre réel positif noté σ. La loi normale de moyenne nulle et d'écart type unitaire {N}(0,1) est appelée loi normale centrée réduite ou loi normale standard.

# Le théorème central limite établit la convergence en loi d'une suite de variables aléatoires vers la loi normale Intuitivement, ce résultat affirme que toute somme de variables aléatoires indépendantes et identiquement distribuées tend vers une variable aléatoire gaussienne.

# En statistiques, un test, ou test d'hypothèse, est une procédure de décision entre deux hypothèses. Il s'agit d'une démarche consistant à rejeter ou à ne pas rejeter une hypothèse statistique.L'hypothèse nulle notée H0 est celle que l'on considère vraie a priori. L’hypothèse H0 est donc privilégiée et il faut des observations très éloignées de cette hypothèse pour la rejeter. Le but du test est de décider si cette hypothèse H0 est a priori crédible.L'hypothèse alternative notée H1 est l'hypothèse complémentaire à l'hypothèse nulle.

# Un test du khi-deux est une méthode de test des hypothèses. Deux tests du khi-deux courants impliquent de vérifier si les fréquences observées dans une ou plusieurs catégories correspondent aux fréquences attendues. Il existe deux tests du khi-deux couramment utilisés : le test du khi-deux de qualité de l'ajustement et le test du khi-deux d'indépendance.Les deux tests impliquent des variables qui divisent vos données en différentes catégories.

# L'ANOVA, qui signifie "analyse de la variance", est un test statistique utilisé pour analyser la différence entre les moyennes de plus de deux groupes.L'hypothèse nulle (H0) de l'ANOVA est qu'il n'y a pas de différence entre les moyennes des groupes. L'hypothèse alternative (Ha) est qu'au moins un groupe diffère significativement de la moyenne générale de la variable dépendante.

# Un scalaire λ est une valeur propre de u s'il existe un vecteur x non nul tel que u(x) = λx.

# Analyse en composante principale : L'analyse en composantes principales est une méthode d'analyse permettant d'explorer de vastes jeux de données multidimensionnels, reposant sur des variables quantitatives.L’ACP est notamment utilisée pour visualiser des corrélations entre les variables, et identifier des groupes homogènes ou à l’inverse des observations atypiques, en particulier des profils à première vue "cachés" à l’intérieur d’un jeu de données

# L'analyse factorielle des correspondances (AFC) est une méthode statistique d'analyse des données qui permet d'analyser et de hiérarchiser les informations contenues dans un tableau rectangulaire de données et qui est aujourd'hui particulièrement utilisée pour étudier le lien entre deux variables qualitatives (ou catégorielles)

# L’analyse des correspondances multiples (ACM) est une méthode d'analyse factorielle adaptée aux données qualitatives (aussi appelées catégorielles).L'AMC généralise l'analyse factorielle des correspondances (AFC), qui étudie le lien entre deux variables qualitatives, en permettant d'étudier le lien entre plusieurs variables qualitatives

# OUTILS ET LIBRAIRIES

# Anaconda est une distribution open-source des langages de programmation Python et R pour la science des données qui vise à simplifier la gestion et le déploiement des packages.Elle fonctionne sous Windows, macOS et Linux. Elle installe, exécute et met à jour rapidement les packages.La distribution Anaconda est livrée avec plus de 250 paquets installés automatiquement.Elle comprend également une interface utilisateur graphique (GUI), Anaconda Navigator, qui constitue une  alternative graphique à l'interface de ligne de commande.

# jupyter Notebook est un environnement de développement web intéractif pour les notebooks, du code et des données.Son interface flexible permet aux utilisateurs de configurer et d'organiser des flux de travail en science des données, en informatique scientifique,en journalisme informatique et en apprentissage automatique. Une conception modulaire invite les extensions pour étendre et enrichir les fonctionnalités.Le nom, Jupyter, vient des principaux langages de programmation qu'il prend en charge : Julia, Python et R.- Jupyter prend en charge plus de 40 langages de programmation, dont Python, R, Julia et Scala.
- Les blocs-notes peuvent être partagés avec d'autres personnes à l'aide du courrier électronique, de Dropbox, de GitHub et de
 la visionneuse de blocs-notes Jupyter.
 - Votre code peut produire une sortie riche et interactive : HTML, images, vidéos, LaTeX et types MIME personnalisés.
- Big data integration
- Intégration de big data : Exploitez les outils de big data, tels qu'Apache Spark, à partir de Python, R et Scala.

# Pandas est une librairie d'analyse et de manipulation de données, puissant, flexible et facile à utiliser,construit sur le langage de programmation Python.Analyser, nettoyer, explorer et manipuler les données constituent ses fonctions Pour utiliser pandas : import pandas

# NumPy est une bibliothèque pour langage de programmation Python, destinée à manipuler des matrices ou tableaux multidimensionnels ainsi que des fonctions mathématiques opérant sur ces tableaux. NumPy est l'acronyme de Numerical Python.NumPy vise à fournir un objet tableau qui est jusqu'à 50 fois plus rapide que les listes Python traditionnelles.
import numpy as np

# Seaborn est une bibliothèque permettant de réaliser des graphiques statistiques en Python. Elle s'appuie sur matplotlib
Seaborn vous aide à explorer et à comprendre vos données. Ses fonctions de traçage opèrent sur des dataframes et des tableaux contenant des ensembles
de données entiers et effectuent en interne la mise en correspondance sémantique et l'agrégation statistique nécessaires pour produire des tracés 
informatifs. Son API déclarative, orientée vers les ensembles de données, vous permet de vous concentrer sur la signification des différents éléments
de vos graphiques, plutôt que sur les détails de leur dessin. et s'intègre étroitement aux structures de données pandas.
On importe la bibliothèque avec la syntaxe : import seaborn as sns
Seaborn et différents types de Dataviz : 
-Les graphiques relationnels pour comprendre les relations entre deux variables
-les graphiques catégoriques pour visualiser des variables classées par catégorie
- les graphiques de distribution pour examiner les distributions univariées ou bivariées
- Les graphiques de régression permettent d’ajouter un guide visuel pour mettre en lumière les motifs dans un ensemble de données pour
 les analyses exploratoires.
* Avantages de Seaborn :
- fourni différents types de visualisations
- outil idéal pour la visualisation statistique (utilisé pour résumer les données dans les visualisations et la distribution des données)
- mieux intégré que matplotlib pour travailler avec les data frames de Pandas

# Matplotlib est une bibliothèque python pour créer des visualisations statiques, animées et interactives.Pour l'utiliser, il faut l'importer ainsi: import matplotlib.pyplot as plt

# Plotly est une librairie Python qui sert à la visualisation et à la compréhension des données facilement.Elle prend en charge plusieuers types de graphes telles que les graphiques linéaires, les nuages de points, les histogrammes,etc. Sa particularité, c'est qu'elle dispose d'outils permettant de détecter toute valeur aberrante ou anomalie dans un grand nombre de points de données. 

# Scikit-learn est la bibliothèque  Python dediée au Machine Learning. Elle propose dans son framework de nombreuses bibliothèques d’algorithmes à implémenter.Il fournit une sélection d'outils efficaces pour l'apprentissage automatique et la modélisation statistique,notamment la classification, la régression, le regroupement et la réduction de la dimensionnalité via une interface de cohérence en Python.Cette bibliothèque, qui est en grande partie écrite en Python, est construite sur NumPy, SciPy et Matplotlib .

# StatsModels est une bibliothèque d’analyse et de modélisation de données statistiques.Elle fournit des classes et des fonctions pour l'estimation de nombreux modèles statistiques différents,ainsi que pour la réalisation de tests statistiques et l'exploration de données statistiques.Parmis ses fonctionnalités, on peut citer :
-la régression linéaire qu'elle complète largement en proposant de nouveaux estimateurs des moindres carrées
-les modèles linéaires généralisés qu'elle permet d'utiliser et qui sont une  généralisation de la régression linéaire classique
- l'étude des séries temporelles 

# Le Nltk ou Natural Language Toolkit est une bibliothèque dédiée au traitement naturel du langage ou Natural Language Processing.Elle  rassemble les algorithmes les plus communs du traitement naturel du langage comme le tokenizing, le part-of-speech tagging,le stemming, l’analyse de sentiment, la segmentation de topic ou la reconnaissance d’entité nommée.

# PyCaret est une bibliothèque open source à faible code en python conçue pour automatiser le développement de modèles d’apprentissage automatique.Elle prend en charge prend en charge l’apprentissage supervisé (classification et régression), le regroupement, la détection d’anomalies et les tâches de traitement du langage naturel. Pycaret regorge différentes fonctionnalités :Préparation des données,Formation des modèles,Réglage des hyperparamètres,Interprétabilité et analyse,Les modèles sont choisis,Enregistrer vos expériences est une bonne idée

# dplyr est constitué principalement d'un ensemble de fonctions conçues pour permettre la manipulation de données de manière intuitive et conviviale.Les cinq fonctions principales permettant diverses formes de manipulation de données :
1- filter(): permet d'extraire des lignes d'un dataframe en fonction de conditions spécifiées par un utilisateur

2- select(): utilisé pour subdiviser en sous-groupe un dataframe à partir de ses colonnes

3- arrange(): pour trier les lignes d'un dataframe en fonction des attributs détenus par des colonnes particulières ; 

4- mutate(): pour créer de nouvelles variables, en modifiant et/ou en combinant les valeurs des colonnes existantes ;

5- summarize(): pour réduire les valeurs d'un dataframe en un seul résumé.

# ggplot2 : c'est une librairie R de visualisation de données basé sur une grammaire des graphiques.Les principes de bases de ggplot2 sont :
1- la notion de esthétique

2- l'organisation en layers (ou niveaux)

# tidyr est un package qui permet de "ranger" facilement les données. Les données ordonnées sont des données avec lesquelles il est facile de travailler : elles sont faciles à fusionner (avec dplyr),à visualiser (avec ggplot2 ou ggvis) et à modéliser (avec les centaines de paquets de modélisation de R). Les deux propriétés les plus importantes des données ordonnées sont les suivantes :
1- Chaque colonne est une variable.

2- Chaque ligne est une observation.
 tidyr fournit trois fonctions principales pour ranger les données : gather(), separate() et spread().

# Le tidyverse est une collection de packages open source pour le langage de programmation R.Les packages de base, qui fournissent des fonctionnalités pour modéliser, transformer et visualiser les données, incluent :ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats

# Shiny est un package R qui facilite la création d'applications web interactives directement à partir de R.Il est possible d'héberger des applications autonomes sur une page web ou les intégrer dans des documents R Markdown ou construire des tableaux de bord. On peut également étendre des applications Shiny avec des thèmes CSS, des htmlwidgets et des actions JavaScript.

# ETL (Extract-Transform-Load) est la norme mondiale pour le traitement des données en masse.
Le processus ETL comprend trois fonctions distinctes :
- Extraction: lors de la phase d’extraction, les données brutes sont extraites d’une variété de sources,
 y compris des bases de données, des outils de réseau, du matériel de sécurité et des applications logicielles, 
entre autres. Ces données affluent dans les réseaux numériques et sont recueillies pratiquement en temps réel.
- Transformation: Lors de la phase de transformation du processus ETL, des flux d’informations sont canalisés sous
forme de données exploitables pour les entreprises. Parallèlement, le moteur ETL réduit le volume des données en détectant
et éliminant les doublons. Les données sont alors normalisées et formatées pour une utilisation et/ou analyse ultérieure.
Enfin, les données sont triées et vérifiées avant de passer à l’étape suivante.
- Chargement: la dernière étape du processus ETL consiste à déposer les données dans les emplacements souhaités. Ces emplacements 
incluent les outils d’analyse, les bases ou les lacs de données, les serveurs de réseau à froid, parmi les différentes utilisations possibles.

Quelques exemples d'ETL : BIRT, Cloudera,Pentaho, Talend

# Une base de données relationnelle est un type de base de données qui stocke et fournit un accès à des points de données liés les uns aux autres. Les bases de données relationnelles sont basées sur le modèle relationnel, un moyen intuitif et simple de représenter des données dans des tables. Dans une base de données relationnelle, chaque ligne de la table est un enregistrement avec un identifiant unique appelé clé. Les colonnes de la table contiennent les attributs des données, et chaque enregistrement a généralement une valeur pour chaque attribut, ce qui facilite l’établissement des relations entre les points de données.

# Etant un outil de data visualisation, Microsoft Power BI est une solution d'analyse de données de Microsoft. Il permet de créer des visualisations de données personnalisées et interactives avec une interface suffisamment simple pour que les utilisateurs finaux créent leurs propres rapports et tableaux de bord. Il fournit des services de BI (business intelligence) hébergés sur le cloud, appelés « services Power BI », ainsi qu'une application de bureau, intitulée « Power BI Desktop ». Plusieurs sources de données peuvent être utilisées telles que des fichiers Excel, des sources SQL, ou des entrepôts de données hybrides locaux ou sur le cloud.

# Tableau est un outil de Data Visualization. L’analyse de données s’effectue très rapidement à l’aide de cet outil.es principales fonctionnalités de Tableau sont l’analyse en temps réel, la visualisation de données et la collaboration.Tableau se connecte à différentes sources pour extraire les données. Il peut s’agir d’une base de données, d’un fichier Excel, d’un PDF et bien plus encore. On distingue deux catégories d’outils Tableau : les outils développeur, et les outils de partage.La première catégorie est utilisée pour le développement de tableaux de bord, de graphiques, de rapports et de visualisation.Il s’agit de Tableau Desktop et Tableau Public.Le logiciel Tableau Server est dédié au partage de workbooks et de visualisations. Une fois téléchargé sur le serveur, le tableau de bord créé avec Tableau Desktop est accessibles aux autres utilisateurs.La plateforme Tableau Online permet le partage en ligne. Ses fonctionnalités sont similaires à Tableau Server, mais les données sont stockées sur les serveurs hébergés sur le Cloud et maintenus par l’éditeur. Cette plateforme se connecte à plus de 40 sources de données comme MySQL,Amazon Aurora ou Spark SQL. Enfin, l’outil gratuit Tableau Reader permet de visionner des visualisations créées avec Tableau Desktop ou public.

# APPRENTISSAGE AUTOMATIQUE

# La science des données est l'étude des données afin d'en extraire des informations significatives pour les entreprises. Il s'agit d'une approche pluridisciplinaire qui combine des principes et des pratiques issus des domaines des mathématiques, des statistiques,de l'intelligence artificielle et du génie informatique, en vue d'analyser de grands volumes de données.Le processus de la science des données repose sur  5 étapes : O- Obtenir les données ;      S- Nettoyer (Scrub) les données ;   E- Explorer les données  ;  M- Modéliser les données  ; N- Interpréter les résultats.

# L'apprentissage automatique est une branche de l'intelligence artificielle (IA) et de l'informatique qui utilise principalement des données et des algorithmes pour imiter la manière dont les être humains apprennent, en améliorant progressivement sa précision.Les algorithmes de Machine Learning apprennent de manière autonome à effectuer une tâche ou à réaliser des prédictions à partir de données et améliorent leurs performances au fil du temps. Les principaux algorithmes de Machine Learning sont :
-Les algorithmes de régression, linéaire ou logistique, permettent de comprendre les relations entre les données.
-L'arbre de décision : Cet algorithme permet d’établir des recommandations basées sur un ensemble de règles de décisions en se basant sur des données classifiées. 
-Enfin, les réseaux de neurones sont des algorithmes se présentant sous la forme d’un réseau à plusieurs couches.
La première couche permet l’ingestion des données, une ou plusieurs couches cachées tirent des conclusions à partir
des données ingérées, et la dernière couche assigne une probabilité à chaque conclusion.

# Le deep learning ou apprentissage profond est un type d'intelligence artificielle dérivé du machine learning (apprentissage automatique) où la machine est capable d'apprendre par elle-même, contrairement à la programmation où elle se contente d'exécuter à la lettre des règles prédéterminées.Il s'appuie sur un réseau de neurones artificiels s'inspirant du cerveau humain. Ce réseau est composé de dizaines voire de centaines de « couches » de neurones, chacune recevant et interprétant les informations de la couche précédente.

# Deep vs Machine Learning
#	 Machine Learning	       Deep Learning

#   Données structurées          Données non structurées
#   controlables                 >1 million de données
#   entrainement par humain      système d'apprent.autonome
#   algorithme modifiable        réseaux neuronal d'algorithme
#   action simple de routine     taches complexes

# Lors de l’apprentissage supervisé,le système s'entraine sur un ensemble de données étiquetées, avec les informations qu’il est censé déterminer.Cette méthode nécessite moins de données d’entraînement que les autres, et facilite le processus d’entraînement puisque les résultats du modèle peuvent être comparés avec les données déjà étiquetées.Tandis que pour l'apprentissage non supervisé,les données n’ont pas d’étiquettes.La machine se contente d’explorer les données à la recherche d’éventuelles patterns. Elle ingère de vastes quantités de données,et utilise des algorithmes pour en extraire des caractéristiques pertinentes requises pour étiqueter, trier et classifier les données en temps réel sans intervention humaine.

# La classification est le type d'apprentissage automatique supervisé.L’idée est alors de classifier les différents éléments d’un jeu de données en plusieurs catégories.Ces dernières regroupent les datas en fonction de leur similarité.Métrique d'évaluation : précision, rappel, score F1

# La régression est un type d'apprentissage automatique supervisé et que le type de sortie est une valeur continue,comme l'âge, la taille, etc. l'un des algorithmes de régression les plus populaires est la régression linéaire....- Métrique d'évaluation : mesure d'erreur, coefficient de détermination

# Les algorithmes de clustering permettent de partitionner les données en sous-groupes, ou clusters, de manière non supervisée.Intuitivement, ces sous-groupes regroupent entre elles des observations similaires.  

# Le NLP pour Natural Language Processing ou Traitement du Langage Naturel est une discipline qui porte essentiellement sur la compréhension,la manipulation et la génération du langage naturel par les machines.  Ainsi, le NLP est réellement à l’interface entre la science informatique et la linguistique. Il porte donc sur la capacité de la machine à interagir directement avec l’humain.Ses applications les plus populaires sont la traduction automatique, le sentiment analysis ou "opinion mind", le marketing, les chatbots.Le NLP comporte deux étapes principales, à savoir le prétraitement des données et le développement de l’algorithme.
-Phase de prétraitement des données : 
1-La récupération du corpus, ansi qu'un premier traitement de ce dernier pour avoir des données textuelles exploitables (au format string).

2-La tokenization, qui désigne le découpage en mots des différents documents qui constituent votre corpus.

3-Le stemming qui est un autre processus de nettoyage des textes. Afin de supprimer les préfixes et les suffixes, il coupe la fin ou le début 
des mots. 

4-La lemmatisation, similairement au stemming, l’idée est de réduire un mot à sa forme de base, ou autrement dit à sa racine. Ensuite,les différentes formes d’un même mot sont regroupées. Cette approche nécessite également des  dictionnaires détaillés dans lesquels l’algorithme
peut rechercher et relier les mots à leurs radicaux. La lemmatisation prend également en compte le contexte du mot pour identifier les différents sens qu’il peut avoir.

5-La suppression des Stop Words :ils signifient également les mots vides. En d‘autres termes, cette technique du NLP vise à supprimer les articles, les pronoms et les prépositions. Elle permet de libérer de l’espace dans la base de données et d’accélérer le temps de traitement.

-Développement de l'algorithme : 
1-La méthode basée sur les règles 

2-La méthode basée sur le Machine Learning

3-La méthode basée sur le deep learning

# La réduction de dimensionnalité en machine learning consiste à réduire le nombre de variables au sein des données d'apprentissage, afin d'obtenir un modèle d'intelligence artificielle plus robuste et un temps de traitement plus rapide. Schématiquement, l'objectif est d'éliminer les variables redondantes ou corrélées. Parmi les algorithmes les plus connus en matière de réduction de dimensionnalité, on peut citer :
PCA (principal component analysis) : l'analyse en composante principale consiste à identifier les principales directions avec des variantes importantes,

LDA (linear discriminant analysis) : l'analyse discriminante linéaire identifie les directions décorrélées les unes des autres,

SVD (singular value decomposition) : la SVD passe par une décomposition d'une matrice en valeurs singulières.

# Le Feature Engineering est un processus qui consiste à transformer les données brutes en caractéristiques représentant plus précisément le problème sous-jacent au modèle prédictif. Pour faire simple,il s’agit d’appliquer une connaissance du domaine pour extraire des représentations analytiques à partir des données brutes et de les préparer pour le Machine Learning.Il existe de multiples approches de features engineering, et celle à adopter dépend du sous-problème spécifique que l’on cherche à résoudre       :
-La  » Feature Importance «  consiste à estimer de manière objective l’utilité d’une caractéristique. Cette démarche peut s’avérer utile pour sélectionner les    caractéristiques 

-L’extraction de caractéristiques ou Feature Extraction permet de construire automatiquement de nouvelles caractéristiques à partir de données brutes. C’est très utile lorsque les observations sont trop volumineuses dans leur forme brute pour être modélisées directement par des algorithmes prédictifs

-La Feature Selection ou sélection de caractéristiques est une autre méthode, permettant de supprimer les attributs des données inutiles ou redondantes dans le contexte du problème à résoudre. Cette approche consiste à sélectionner automatiquement le sous-ensemble le plus utile pour la résolution du dit problème.

-Le Feature Learning ou apprentissage de caractéristiques consiste à identifier et à utiliser automatiquement les caractéristiques des données
brutes.Le but est d’éviter d’avoir à construire ou extraire des caractéristiques manuellement.

# La validation croisée (« cross-validation ») est, en apprentissage automatique, une méthode d’estimation de fiabilité d’un modèle fondée sur une technique d’échantillonnage.

# données d'entrainement : Les données d’entraînement sont celles qui permettront à votre IA d’apprendre à effectuer la tâche qui lui est confiée.Elles seront utilisées encore et encore afin d’améliorer ses prédictions et son taux de réussite.L’IA utilisera ces données d’entraînement de plusieurs façons dans le but d’améliorer la précision de ses prédictions.La plupart des données d’entraînement contiennent des paires input / étiquette.

# données de validation : Un jeu de données de validation est un jeu de données d'exemples utilisés pour régler les hyperparamètres (c'est-à-dire l'architecture) d'un classifieur.Un exemple d'hyperparamètre pour les réseaux de neurones artificiels comprend le nombre d'unités cachées dans chaque couche. 

# données de test : Un jeu de données de test est un jeu de données indépendant du jeu de données d'apprentissage, mais qui suit la même distribution de probabilité que le jeu de données d'apprentissage.Il est donc un jeu d'exemples utilisés uniquement pour évaluer les performances (c'est-à-dire la généralisation) d'un classificateur entièrement spécifié.

#  ASPECT METIER

# Data scientist :à l'intersection entre data analyst et data enginerre,c'est un profil pluridisciplinaire qui aura pour mission première de tirer de l’information utile (insights) depuis des données brutes.En effet, un Data scientist va explorer et exploiter les gisements de données de l’entreprise pour leur appliquer des techniques de machine learning.

# Data analyst : Exploitation et interprétation des données pour en dégager des observations business utiles. Ainsi, les rapports fournis permettent d'orienter les prises de décision du Management et améliorer les performances et les stratégies Marketing.

# Data engineer : premier acteur du traitement de la donnée, Les Data Engineer vont collecter, transformer les données de différentes sources. Ce travail préparatoire permettra d’avoir des données “propres”, prêtes pour qu’on leur applique dessus des techniques de Machine Learning.

# Différence entre ces trois : Niveau de traitement des données
Niv 1: data engineer: Développer, tester et maintenir les architectures 

Niv 2: data analyst : Prétraitement et collecte de données

Niv 3: data scientist : Responsable du développement des modèles opérationnels

# SITOGRAPHIE

# https://fr.wikipedia.org/wiki/Matrice_   
# https://frwikipedia.org/wiki/Vecteur#
# https://www.lemagit.fr/definition/Variable-independante#
# https://fr.khanacademy.org/
# https://fr.wikipedia.org/wiki/%C3%89cart_type
# https://fr.wikipedia.org/wikiEsp%C3%A9rance_math%C3%A9matique
# https://fr.wikipedia.org/wiki/Variance_(math%C3%A9matiques
# https://fr.wikipedia.org/wiki/Corr%C3%A9lation(statistiques)
# https://www.insee.fr/fr/metadonnees/
# https://fr.wikipedia.org/wiki/M%C3%A9diane_(statistiques)
# https://www.pfi-culture.org/wp-content/uploads/sites/1052/2016/04/0ecochtermesstat.pdf
# https://commentprogresser.com/statistique-parametre-statistiques-moyenne-mediane-etendue-ecart-type.html#moyenne
# https://www.stat4decision.com/fr/le-box-plot-ou-la-fameuse-boite-a-moustache
# https://www150.statcan.gc.ca/n1/edu/power-pouvoir/ch9/histo/5214822-fra.htm
# https://en.wikipedia.org/wiki/Loss_function
# https://fr.wikipedia.org/wiki/D%C3%A9riv%C3%A9e
# https://machinelearnia.com/descente-de-gradient
# https://fr.wikipedia.org/wiki/Loi_normale
# https://www.techno-science.net/glossaire-definition/Theoreme-central-limite.html
# https://fr.wikipedia.org/wiki/Test_statistique
# https://www.jmp.com/fr_fr/statistics-knowledge-portal/chi-square-test.html
# https://www.scribbr.com/statistics/one-way-anova/
# https://www.journaldunet.fr/web-tech/guide-de-l-intelligence-artificielle/1501303-analyse-en-composantes-principales-acp-definition-et-cas-d-usage/
# https://fr.wikipedia.org/wiki/Analyse_factorielle_des_correspondances
# https://fr.wikipedia.org/wiki/Analyse_des_correspondances_multiples
# https://jupyter.org/
# https://www.dominodatalab.com
# https://pandas.pydata.org/
# https://seaborn.pydata.org/  https://datascientest.com/
#  https://matplotlib.org/
# https://www.statsmodels.org/   https://datascientest.com/statsmodels
# https://datascientest.com/nltk
# wikipedia.org
#  https://posit.co/
# https://shiny.rstudio.com/
# https://datascientest.com/tableau-outil-de-business-intelligence
# https://aws.amazon.com/fr/what-is/data-science/
# https://datascientest.com/machine-learning-tout-savoir   https://www.ibm.com/fr-fr/cloud/learn/machine-learning
# https://www.ionos.fr
# https://datascientest.com/machine-learning-tout-savoir
# https://intelligence-artificielle.com/nlp-guide-complet
# https://datascientest.com/introduction-au-nlp-natural-language-processing
# https://www.journaldunet.fr/web-tech/guide-de-l-intelligence-artificielle/1501907-reduction-de-dimensionnalite/
# https://datascientest.com/feature-engineering
#  https://fr.wikipedia.org/wiki/Validation_crois%C3%A9e
# https://www.lebigdata.fr/machine-learning-entrainement-ia
# https://fr.wikipedia.org/wiki/Jeux_d%27entrainement,_de_validation_et_de_test
# 





